{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 1 Notebook and Report\n",
    "## I. First step by step guide to using a Trained Model\n",
    "First thing to do would be to load the model. \n",
    "\n",
    "Information on how to use the repository are provided in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load(\"model_checkpoint\\BiLSTMConcat_checkpoint_epoch5.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is used, it is possible to give him two sentences to predict their relationship between entailment, neutral and contradiction.\n",
    "But first, the data need to be pre-processed. Let's take two sentences : \n",
    "\n",
    "\"Harvey Specter is solving a case and he thinks about bribing the police.\"\n",
    "\"Harvey corrupts the police.\"\n",
    "\n",
    "The premise entails the hypothesis so the label predicted by the model should be 0, as entailment = 0, neutral = 1 and contradiction = 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35960,     0,    12,  8140,     2,  2828,    26,    73,  7664,   761,\n",
      "             0,    35,  2271,    11]], device='cuda:0')\n",
      "tensor([[35960,     0,    35,  2271,    11]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "premise = \"Harvey Specter is solving a case and he thinks about bribing the police.\"\n",
    "hypothesis = \"Harvey corrupts the police.\"\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "# First we tokenize the sentences\n",
    "premise = nltk.tokenize.word_tokenize(premise)\n",
    "hypothesis = nltk.tokenize.word_tokenize(hypothesis)\n",
    "\n",
    "#Then we lowercase all the tokens\n",
    "premise = [words.lower() for words in premise]\n",
    "hypothesis = [words.lower() for words in hypothesis]\n",
    "\n",
    "# We load the vocabulary\n",
    "with open(\"data/data.json\", 'r') as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "from data import prepare_example\n",
    "\n",
    "#This function will assign indexes to token, so that they are recognizes by the embedding table that was used to train the model.\n",
    "premise = prepare_example(premise, vocab)\n",
    "hypothesis = prepare_example(hypothesis, vocab)\n",
    "\n",
    "print(premise)\n",
    "print(hypothesis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing the cell below, you can see that the model gives higher probability to the first class, which refers to entailment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9438, 0.0391, 0.0171]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "print(softmax(model(premise, hypothesis)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Why does the model sometimes fail ?\n",
    "\n",
    "Premise - “Two men sitting in the sun”\n",
    "Hypothesis - “Nobody is sitting in the shade”\n",
    "\n",
    "Label - Neutral (likely predicts contradiction)\n",
    "\n",
    "Premise - “A man is walking a dog”\n",
    "Hypothesis - “No cat is outside”\n",
    "\n",
    "Label - Neutral (likely predicts contradiction)\n",
    "\n",
    "Can you think of a possible reason why the model would fail in such cases?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  83,  452,  102,   41,   35, 1370]], device='cuda:0')\n",
      "tensor([[   2,   85,   12,  102,   41,   35, 3608]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "premise_1 = \"Two men sitting in the sun\"\n",
    "hypothesis_1 = \"Nobody is sitting in the shade\"\n",
    "\n",
    "premise_2 = \"A man is walking a dog\"\n",
    "hypothesis_2 = \"No cat is outside\"\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "# First we tokenize the sentences\n",
    "premise_1 = nltk.tokenize.word_tokenize(premise_1)\n",
    "hypothesis_1 = nltk.tokenize.word_tokenize(hypothesis_1)\n",
    "\n",
    "premise_2 = nltk.tokenize.word_tokenize(premise_2)\n",
    "hypothesis_2 = nltk.tokenize.word_tokenize(hypothesis_2)\n",
    "\n",
    "#Then we lowercase all the tokens\n",
    "premise_1 = [words.lower() for words in premise_1]\n",
    "hypothesis_1 = [words.lower() for words in hypothesis_1]\n",
    "\n",
    "premise_2 = [words.lower() for words in premise_2]\n",
    "hypothesis_2 = [words.lower() for words in hypothesis_2]\n",
    "\n",
    "# We load the vocabulary\n",
    "with open(\"data/data.json\", 'r') as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "from data import prepare_example\n",
    "\n",
    "#This function will assign indexes to token, so that they are recognizes by the embedding table that was used to train the model.\n",
    "premise_1 = prepare_example(premise_1, vocab)\n",
    "hypothesis_1 = prepare_example(hypothesis_1, vocab)\n",
    "\n",
    "premise_2 = prepare_example(premise_2, vocab)\n",
    "hypothesis_2 = prepare_example(hypothesis_2, vocab)\n",
    "\n",
    "print(premise_1)\n",
    "print(hypothesis_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For premise_1 and hypothesis_1, model predicts : \n",
      "tensor([[1.3425e-06, 2.0507e-05, 9.9998e-01]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "For premise_2 and hypothesis_2, model predicts : \n",
      "tensor([[1.6535e-07, 6.9126e-07, 1.0000e+00]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "print(\"For premise_1 and hypothesis_1, model predicts : \")\n",
    "print(softmax(model(premise_1, hypothesis_1)))\n",
    "\n",
    "print(\"For premise_2 and hypothesis_2, model predicts : \")\n",
    "print(softmax(model(premise_2, hypothesis_2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model predicts contradiction for these two sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In those two last examples, the model is quite certain that the right label is contradiction. Moreover, it has to be specified that the loaded model is our best performing model on SNLI and SentEVal with an accuracy approximating 85% on both tasks. \n",
    "\n",
    "The first thing that I did is comparing the typical sentences of the SNLI dataset with the premises and hypothesis provided. One could say that the premises of the SNLi dataset tend to be longer and more detailed but some examples match the length of the two provided sentences.\n",
    "\n",
    "One reason why the model fail could be the negation of the premises' subjects. In both hypothesis, the following \"Nobody\" and \"No cat\" are employed as the subject of the sentence. It could be that the model associates negation with contradiction and thus predict the wrong label.\n",
    "\n",
    "To experiment and see if that is the reason, I modified the last example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise_2 = \"A man is walking a dog\"\n",
    "hypothesis_2 = \"A cat is outside\"\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "# First we tokenize the sentences\n",
    "premise_2 = nltk.tokenize.word_tokenize(premise_2)\n",
    "hypothesis_2 = nltk.tokenize.word_tokenize(hypothesis_2)\n",
    "\n",
    "#Then we lowercase all the tokens\n",
    "premise_2 = [words.lower() for words in premise_2]\n",
    "hypothesis_2 = [words.lower() for words in hypothesis_2]\n",
    "\n",
    "# We load the vocabulary\n",
    "with open(\"data/data.json\", 'r') as file:\n",
    "    vocab = json.load(file)\n",
    "\n",
    "from data import prepare_example\n",
    "\n",
    "#This function will assign indexes to token, so that they are recognizes by the embedding table that was used to train the model.\n",
    "premise_2 = prepare_example(premise_2, vocab)\n",
    "hypothesis_2 = prepare_example(hypothesis_2, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For premise_2 and hypothesis_2, model predicts : \n",
      "tensor([[0.1744, 0.8138, 0.0118]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "print(\"For premise_2 and hypothesis_2, model predicts : \")\n",
    "print(softmax(model(premise_2, hypothesis_2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model still predicts contradiction even without negation. For this particular sentence, I also thought of the fact that cat and dog are close in the embedding space and might be used to being opposite, hence the contradiction. I then tried the sentences \"No crocodile is outside\" and \" A crocodile is outside\" as hypothesis but both are predicted as a contradiction. I did the same experiences with the first set of sentences, replacing \"Nobody\" by \"A women\" or \"No women\" and all examples lead to contradiction. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OIn a large majority of the dataset examples, the subject of the hypothesis is either directly refered by the same word as in the premise, or refered by a \"similar\" word, as \"old guy\" could be refered by \"man\". However, the subjects in the given hypothesis here do not appear in the premises, which is an uncommun sentence structure. that might be the reason why the model fails."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
